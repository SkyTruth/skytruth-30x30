{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method used to calculate global protected areas and OECM coverage\n",
    "\n",
    "The protected area coverage is calculated (from WDPA [methodolgy](https://www.protectedplanet.net/en/resources/calculating-protected-area-coverage)):\n",
    "\n",
    "1. Start with the latest WDPA monthly release\n",
    "2. The WDPA is filtered to exclude records with the characteristics listed in Section 2\n",
    "3. A buffer is created around protected areas reported as points using their Reported Area. There are important caveats associated with this method, some of which are explored by Visconti et al. 2013. Buffering points can underestimate or overestimate protected area coverage as the circles created around points might cover areas where protected areas do not exist (overestimation) or overlap with areas where other protected areas already exist (underestimation). It can also give inaccurate values for sites that are partly terrestrial and marine as the absence of boundaries make it difficult to predict which portion of a protected area is in the land or the sea.\n",
    "4. Both polygon and buffered point layers are combined in a single layer\n",
    "5. The layer above is flattened (dissolved) â€“ to eliminate overlaps between designations and avoid double counting.\n",
    "6. The global protected areas flat layer is intersected with a base map of the world (see Section 3)\n",
    "7. The intersected flat layer is converted to Mollweide (an equal area projection) and the area of each polygon is calculated, in km2.\n",
    "8. Calculated areas are summed by land, marine and Areas Beyond National Jurisdiction (ABNJ). Marine and coastal area are those outlined in the Economic Exclusion Zones dataset (see Section 3 above). ABNJ constitute international waters outside the 200 nautical mile limits of national jurisdiction.\n",
    "9. The terrestrial protected area coverage is calculated by dividing the total area of terrestrial protected areas by total global terrestrial area excluding Antarctica. ABNJ protected area coverage is calculated by selecting areas where ISO3 = 'ABNJ'. Marine and coastal protected area coverage is total global protected areas flat coverage - (ABNJ Area + Land Area)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_in = \"/Users/sofia/Documents/Repos/skytruth-30x30/data/data/raw\"\n",
    "path_out = \"/Users/sofia/Documents/Repos/skytruth-30x30/data/data/processed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download and import last release of [MPA](https://www.protectedplanet.net/en/thematic-areas/marine-protected-areas): Sept 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "curl 'https://www.protectedplanet.net/downloads' \\\n",
    "  -H 'authority: www.protectedplanet.net' \\\n",
    "  -H 'accept: application/json, text/plain, */*' \\\n",
    "  -H 'accept-language: en,es;q=0.9,gl;q=0.8' \\\n",
    "  -H 'cache-control: no-cache' \\\n",
    "  -H 'content-type: application/json;charset=UTF-8' \\\n",
    "  -H 'cookie: _ProtectedPlanet_session=L0k1ZS9vaFdhbndhTkl6ZVkxd2Vza1dvNFU3NGtVbUVQNzA0SmNRTGRZNnRSMlpyeEhCNDd2czdnRkFiVFAyNUVpYzlpNFBEQmFpZG9vbjl3UlczMlRqU0ZRakRhYkJ3RmhFTy9UTWRVR3hsN1JIWnpUdTBmMWFyYVdGYVE1SWJSeWFXcnM1SlRIcUVWZk9MZ2psUXFnPT0tLWZkL2JyS0pMZmJqOUdXRlV0Z3h6OVE9PQ%3D%3D--e6ef3f79fc989a45cc1966990742d20065f88b76' \\\n",
    "  -H 'dnt: 1' \\\n",
    "  -H 'origin: https://www.protectedplanet.net' \\\n",
    "  -H 'pragma: no-cache' \\\n",
    "  -H 'referer: https://www.protectedplanet.net/en/thematic-areas/marine-protected-areas' \\\n",
    "  -H 'sec-ch-ua: \"Chromium\";v=\"118\", \"Google Chrome\";v=\"118\", \"Not=A?Brand\";v=\"99\"' \\\n",
    "  -H 'sec-ch-ua-mobile: ?0' \\\n",
    "  -H 'sec-ch-ua-platform: \"Linux\"' \\\n",
    "  -H 'sec-fetch-dest: empty' \\\n",
    "  -H 'sec-fetch-mode: cors' \\\n",
    "  -H 'sec-fetch-site: same-origin' \\\n",
    "  -H 'user-agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36' \\\n",
    "  -H 'x-csrf-token: ISjsyNxbS5mkzQSQCmEzbcPltwt6SMOOV0zertwP07Lc+oIzrR9McY4JXvj17mkZ9tSd3JIoCaFfnRcC/eTWzw==' \\\n",
    "  --data-raw '{\"domain\":\"general\",\"format\":\"shp\",\"token\":\"marine\",\"id\":21961}' \\\n",
    "  --compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly1 = gpd.read_file(path_in + \"/WDPA_WDOECM_Sep2023_Public_marine_shp/WDPA_WDOECM_Sep2023_Public_marine_shp_0/WDPA_WDOECM_Sep2023_Public_marine_shp-polygons.shp\")\n",
    "point1 = gpd.read_file(path_in + \"/WDPA_WDOECM_Sep2023_Public_marine_shp/WDPA_WDOECM_Sep2023_Public_marine_shp_0/WDPA_WDOECM_Sep2023_Public_marine_shp-points.shp\")\n",
    "poly2 = gpd.read_file(path_in + \"/WDPA_WDOECM_Sep2023_Public_marine_shp/WDPA_WDOECM_Sep2023_Public_marine_shp_1/WDPA_WDOECM_Sep2023_Public_marine_shp-polygons.shp\")\n",
    "point2 = gpd.read_file(path_in + \"/WDPA_WDOECM_Sep2023_Public_marine_shp/WDPA_WDOECM_Sep2023_Public_marine_shp_1/WDPA_WDOECM_Sep2023_Public_marine_shp-points.shp\")\n",
    "poly3 = gpd.read_file(path_in + \"/WDPA_WDOECM_Sep2023_Public_marine_shp/WDPA_WDOECM_Sep2023_Public_marine_shp_2/WDPA_WDOECM_Sep2023_Public_marine_shp-polygons.shp\")\n",
    "point3 = gpd.read_file(path_in + \"/WDPA_WDOECM_Sep2023_Public_marine_shp/WDPA_WDOECM_Sep2023_Public_marine_shp_2/WDPA_WDOECM_Sep2023_Public_marine_shp-points.shp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6033\n",
      "172\n",
      "6033\n",
      "172\n",
      "6033\n",
      "171\n"
     ]
    }
   ],
   "source": [
    "print(len(poly1))\n",
    "print(len(point1))\n",
    "print(len(poly2))\n",
    "print(len(point2))\n",
    "print(len(poly3))\n",
    "print(len(point3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Filter WDPA to exclude records:\n",
    "- \"Non Reported\" protected areas (methodology recommends to remove also \"Proposed\" but we keep it for future projections)\n",
    "- MAB (Note: MAB sites reported as OECMs are included in coverage analyses)\n",
    "- Sites submitted as points with no reported area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [poly1, point1, poly2, point2, poly3, point3]\n",
    "\n",
    "for i, df in enumerate(dataframes):\n",
    "    # Remove rows where 'status' is equal to 'Not Reported'\n",
    "    df = df[df['STATUS'] != 'Not Reported']\n",
    "\n",
    "    # Remove rows where 'DESIG' contains 'MAB'\n",
    "    df = df[~df['DESIG_ENG'].str.contains('MAB', case=False)]\n",
    "\n",
    "    # Check if the dataframe is one of point1, point2, or point3\n",
    "    if i in [1, 3, 5]:\n",
    "        # Remove rows where reported area is 0\n",
    "        df = df[(df['REP_AREA'] != 0)]\n",
    "\n",
    "    # Update the original dataframes in the list\n",
    "    dataframes[i] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5999\n",
      "157\n",
      "6018\n",
      "123\n",
      "6014\n",
      "135\n"
     ]
    }
   ],
   "source": [
    "print(len(dataframes[0]))\n",
    "print(len(dataframes[1]))\n",
    "print(len(dataframes[2]))\n",
    "print(len(dataframes[3]))\n",
    "print(len(dataframes[4]))\n",
    "print(len(dataframes[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create buffers around points based on reported area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate radius based on REP_AREA\n",
    "def calculate_radius(rep_area):\n",
    "    return (rep_area / 3.14159265358979323846) ** 0.5\n",
    "\n",
    "# Iterate through the list and process the desired dataframes\n",
    "for idx in [1, 3, 5]:\n",
    "    # Get the dataframe at the specified index\n",
    "    gdf = dataframes[idx]\n",
    "\n",
    "    # Reproject in Mollweide\n",
    "    gdf = gdf.to_crs('ESRI:54009')\n",
    "\n",
    "    # Transform the reported area from square kilometers to square meters\n",
    "    gdf['REP_AREA_m'] = gdf['REP_AREA'] * 1000000\n",
    "\n",
    "    # Create the \"radius\" column by applying the calculate_radius function to the \"REP_AREA\" column\n",
    "    gdf['radius'] = gdf['REP_AREA_m'].apply(calculate_radius)\n",
    "\n",
    "    # Create buffers around the points using the \"radius\" column\n",
    "    gdf_buffered = gdf.copy()\n",
    "    gdf_buffered['geometry'] = gdf.apply(lambda row: row.geometry.buffer(row['radius']), axis=1)\n",
    "\n",
    "    # Reproject back to WGS84\n",
    "    gdf_buffered = gdf_buffered.to_crs('EPSG:4326')\n",
    "\n",
    "    # Remove rows with invalid geometries\n",
    "    gdf_buffered = gdf_buffered[gdf_buffered['geometry'].is_valid]\n",
    "\n",
    "    # Update the original dataframe with the buffered data\n",
    "    dataframes[idx] = gdf_buffered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Merge the 6 datasets (polygons and buffered points) in a single layer and segregate those that are \"Proposed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All gdf have the same crs: EPSG:4326\n"
     ]
    }
   ],
   "source": [
    "# Check that all of them have the same crs\n",
    "first_crs = dataframes[0].crs\n",
    "same_crs = all(gdf.crs == first_crs for gdf in dataframes[1:])\n",
    "if same_crs:\n",
    "    print(\"All gdf have the same crs:\", first_crs)\n",
    "else:\n",
    "        print(\"gdf have different crs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18445"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge dataframes\n",
    "merged_mpa = pd.concat(dataframes)\n",
    "len(merged_mpa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if geometries are valid\n",
    "sum(merged_mpa.geometry.is_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save merged_mpa as a shapefile\n",
    "merged_mpa.to_file(path_out + \"/wdpa/merged_mpa.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separate \"Proposed\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the rows where 'STATUS' is equal to 'Proposed'\n",
    "proposed = merged_mpa[merged_mpa['STATUS'] == 'Proposed']\n",
    "\n",
    "# Select only the rows where 'STATUS' is different from 'Proposed'\n",
    "protected = merged_mpa[merged_mpa['STATUS'] != 'Proposed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the two dataframes as shapefiles\n",
    "proposed.to_file(path_out + \"/wdpa/proposed.shp\")\n",
    "protected.to_file(path_out + \"/wdpa/protected.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(proposed))\n",
    "print(len(protected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Dissolve intersecting polygons by relevant fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "from typing import  Union, Dict\n",
    "import sys\n",
    "import time\n",
    "from mapshaper import Mapshaper\n",
    "import multiprocessing as mp\n",
    "import psutil\n",
    "\n",
    "scripts_dir = Path(\"..\").joinpath(\"src\")\n",
    "if scripts_dir not in sys.path:\n",
    "    sys.path.insert(0, scripts_dir.resolve().as_posix())\n",
    "\n",
    "from pipelines.utils import load_regions\n",
    "from pipelines.output_schemas import ProtectedAreaExtentSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Path(\"../data/mpa_intermediate\").resolve()\n",
    "intermediate_file = data.joinpath(\"mpa_intermediate\", \"mpa_intermediate.shp\").as_posix()\n",
    "output_path = data.joinpath(\"timeseries\")\n",
    "\n",
    "output_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "rename_dict = {\n",
    "    \"cumSumProt\": \"cumsum_area\",\n",
    "    \"protectedA\": \"protectedAreasCount\",\n",
    "    \"PA_DEF\": \"protection_status\",\n",
    "}\n",
    "location_index = Path(\"../data/eez_intermediate\").joinpath(\"locations_code.csv\").as_posix()\n",
    "\n",
    "CPU_COUNT = mp.cpu_count()\n",
    "AVAILABLE_MEMORY = 64 * 1024 * 1024 * 1024  # 64 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2002, 2023):\n",
    "    Mapshaper(16).input(files=[intermediate_file]).filter(\n",
    "        expression=f\"'STATUS_YR<={year}'\"\n",
    "    ).dissolve2(\n",
    "        fields=\"'PA_DEF,PARENT_ISO'\", calc=\"'protectedAreasCount = count()'\"\n",
    "    ).explode().reproject(\n",
    "        \"+proj=moll +lon_0=0 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs +type=crs\"\n",
    "    ).each(\n",
    "        expression=\"cumSumProtectedArea=this.area/1000000\"\n",
    "    ).output(\n",
    "        output_path.joinpath(f\"protected_dissolved_{year}.shp\").as_posix()\n",
    "    ).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate coverage statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_coverage_calculation(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    global_area = (\n",
    "        df.groupby([\"PA_DEF\", \"year\", \"PARENT_ISO\"])\n",
    "        .agg(\n",
    "            {\n",
    "                \"cumSumProt\": \"sum\",\n",
    "                \"protectedA\": \"first\",\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "        .groupby([\"PA_DEF\", \"year\"])\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "        .assign(PARENT_ISO=\"GLOB\")\n",
    "    )\n",
    "    return pd.concat(\n",
    "        [\n",
    "            df.replace(\n",
    "                {\n",
    "                    \"PARENT_ISO\": {\n",
    "                        \"COK\": \"NZL\",\n",
    "                        \"IOT\": \"GBR\",\n",
    "                        \"NIU\": \"NZL\",\n",
    "                        \"SHN\": \"GBR\",\n",
    "                        \"SJM\": \"NOR\",\n",
    "                        \"UMI\": \"USA\",\n",
    "                    }\n",
    "                }\n",
    "            ),\n",
    "            global_area,\n",
    "        ],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def separate_parent_iso(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"PARENT_ISO\"] = df[\"PARENT_ISO\"].str.split(\";\")\n",
    "    return df.explode(\"PARENT_ISO\")\n",
    "\n",
    "\n",
    "def aggregate_area(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return (\n",
    "        df.groupby([\"PA_DEF\", \"PARENT_ISO\", \"year\"])\n",
    "        .agg({\"cumSumProt\": \"sum\", \"protectedA\": \"first\"})\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "\n",
    "def add_region_iso(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    regions = load_regions()\n",
    "\n",
    "    def find_region_iso(iso: str) -> Union[str, None]:\n",
    "        filtered_regions = list(filter(lambda x: iso in x[\"country_iso_3s\"], regions.get(\"data\")))\n",
    "        return filtered_regions[0][\"region_iso\"].strip() if len(filtered_regions) > 0 else None\n",
    "\n",
    "    return df.assign(region=lambda row: row[\"PARENT_ISO\"].apply(find_region_iso))\n",
    "\n",
    "\n",
    "def region_coverage_calculation(df: pd.DataFrame):\n",
    "    regions = (\n",
    "        df.pipe(add_region_iso)\n",
    "        .groupby([\"PA_DEF\", \"region\", \"year\"])\n",
    "        .agg({\"cumSumProt\": \"sum\", \"protectedA\": \"sum\"})\n",
    "        .reset_index()\n",
    "    )\n",
    "    return pd.concat(\n",
    "        [df, regions.assign(PARENT_ISO=lambda row: row[\"region\"]).drop(columns=\"region\")],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def output(df: pd.DataFrame, cols: Dict[str, str]) -> pd.DataFrame:\n",
    "    locations_code = pd.read_csv(location_index, keep_default_na=False)\n",
    "    return (\n",
    "        df.join(locations_code.set_index(\"code\"), on=\"PARENT_ISO\", how=\"left\")\n",
    "        .replace({\"PA_DEF\": {\"0\": 2, \"1\": 1}})\n",
    "        .rename(columns=cols)\n",
    "        .drop(columns=[\"PARENT_ISO\", \"cumsum_area\"])\n",
    "        .assign(\n",
    "            id=df.index + 1,\n",
    "            cumSumProtectedArea=df.cumSumProt.round(2),\n",
    "            protectedArea=(\n",
    "                df.sort_values(by=[\"PARENT_ISO\", \"year\", \"PA_DEF\"]).cumSumProt\n",
    "                - df.sort_values(by=[\"PARENT_ISO\", \"year\", \"PA_DEF\"])\n",
    "                .groupby([\"PA_DEF\", \"PARENT_ISO\", \"year\"])\n",
    "                .cumSumProt.shift(-1, fill_value=0)\n",
    "                .reset_index(drop=True)\n",
    "            ).round(2),\n",
    "        )\n",
    "        .set_index(\"id\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'joinpath'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/alicitita/Projects/skytruth-30x30/data/notebooks/wdpa_coverage.ipynb Cell 29\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alicitita/Projects/skytruth-30x30/data/notebooks/wdpa_coverage.ipynb#Y115sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m does not exist\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alicitita/Projects/skytruth-30x30/data/notebooks/wdpa_coverage.ipynb#Y115sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m final \u001b[39m=\u001b[39m (\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alicitita/Projects/skytruth-30x30/data/notebooks/wdpa_coverage.ipynb#Y115sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     pd\u001b[39m.\u001b[39mconcat(data, ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mpipe(region_coverage_calculation)\u001b[39m.\u001b[39mpipe(output, rename_dict)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alicitita/Projects/skytruth-30x30/data/notebooks/wdpa_coverage.ipynb#Y115sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alicitita/Projects/skytruth-30x30/data/notebooks/wdpa_coverage.ipynb#Y115sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m ProtectedAreaExtentSchema(final)\u001b[39m.\u001b[39mto_csv(\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/alicitita/Projects/skytruth-30x30/data/notebooks/wdpa_coverage.ipynb#Y115sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     data\u001b[39m.\u001b[39;49mjoinpath(\u001b[39m\"\u001b[39m\u001b[39mprotected_area_extent.csv\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mas_posix(), index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alicitita/Projects/skytruth-30x30/data/notebooks/wdpa_coverage.ipynb#Y115sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'joinpath'"
     ]
    }
   ],
   "source": [
    "years_rage = range(2000, time.localtime().tm_year)\n",
    "# dissolve Mpas subtables\n",
    "data_list = []\n",
    "for year in years_rage:\n",
    "    file = output_path.joinpath(f\"protected_dissolved_{year}.shp\")\n",
    "    if file.exists():\n",
    "        data_list.append(\n",
    "            (\n",
    "                gpd.read_file(file)\n",
    "                .assign(year=year)\n",
    "                .drop(columns=[\"geometry\"])\n",
    "                .pipe(global_coverage_calculation)\n",
    "                .pipe(separate_parent_iso)\n",
    "                .pipe(aggregate_area)\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        print(f\"{file} does not exist\")\n",
    "\n",
    "final = (\n",
    "    pd.concat(data_list, ignore_index=True)\n",
    "    .pipe(region_coverage_calculation)\n",
    "    .pipe(output, rename_dict)\n",
    ")\n",
    "\n",
    "ProtectedAreaExtentSchema(final).to_csv(\n",
    "    data.resolve().joinpath(\"protected_area_extent.csv\").as_posix(),\n",
    "    index=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
