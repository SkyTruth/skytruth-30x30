{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Tuple, List\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import asyncio\n",
    "from tqdm.asyncio import tqdm\n",
    "from itertools import product\n",
    "from shapely.geometry import box\n",
    "\n",
    "\n",
    "scripts_dir = Path(\"..\").joinpath(\"src\")\n",
    "if scripts_dir not in sys.path:\n",
    "    sys.path.insert(0, scripts_dir.resolve().as_posix())\n",
    "\n",
    "from pipelines.utils import background\n",
    "from pipelines.processors import calculate_area, get_matches, repair_geometry, arrange_dimensions, clean_geometries, simplify_async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logging.getLogger(\"requests\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"fiona\").setLevel(logging.WARNING)\n",
    "logger = logging.getLogger(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_year(\n",
    "    gdf: gpd.GeoDataFrame, year_col: str = \"STATUS_YR\", year_val: int = 2010\n",
    ") -> List[gpd.GeoDataFrame]:\n",
    "    \"\"\"Split data by year. relevant for MPA data.(coverage indicator)\"\"\"\n",
    "    prior_2010 = (\n",
    "        gdf[gdf[year_col] <= year_val][[\"iso_3\", \"STATUS_YR\", \"geometry\"]]\n",
    "        .dissolve(\n",
    "            by=[\"iso_3\"],\n",
    "        )\n",
    "        .assign(year=2010)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    after_2010 = (\n",
    "        gdf[gdf[\"STATUS_YR\"] > 2010][[\"iso_3\", \"STATUS_YR\", \"geometry\"]]\n",
    "        .rename(columns={\"STATUS_YR\": \"year\"})\n",
    "    )\n",
    "    return [prior_2010, after_2010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grid(bounds: Tuple[float, float, float, float], cell_size: int = 1) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Create a grid of cells for a given GeoDataFrame\"\"\"\n",
    "    minx, miny, maxx, maxy = bounds\n",
    "    x = np.arange(minx, maxx, cell_size)\n",
    "    y = np.arange(miny, maxy, cell_size)\n",
    "    polygons = [\n",
    "        {\n",
    "            \"geometry\": box(i, j, i + cell_size, j + cell_size),\n",
    "            \"cell_id\": f\"{i}_{j}\",\n",
    "        }\n",
    "        for i, j in product(x, y)\n",
    "    ]\n",
    "    return gpd.GeoDataFrame(polygons)\n",
    "\n",
    "\n",
    "def subdivide_grid(\n",
    "    grid_gdf: gpd.GeoDataFrame, gdf: gpd.GeoDataFrame, max_cellsize: float, max_complexity: int\n",
    ") -> List:\n",
    "    subdivided_elements = []\n",
    "    for grid_element in grid_gdf.geometry:\n",
    "        candidates = get_matches(grid_element, gdf)\n",
    "        density = len(candidates)\n",
    "        if density > max_complexity:\n",
    "            \n",
    "            subdivision_cellsize = max_cellsize / 2\n",
    "            # Subdivide the grid element recursively\n",
    "            subgrid = create_grid(grid_element.bounds, subdivision_cellsize)\n",
    "            subdivided_elements.extend(\n",
    "                subdivide_grid(subgrid, gdf, subdivision_cellsize, max_complexity)\n",
    "            )\n",
    "        elif density > 0:\n",
    "            subdivided_elements.append(grid_element)\n",
    "\n",
    "    return subdivided_elements\n",
    "\n",
    "\n",
    "def create_density_based_grid(\n",
    "    gdf: gpd.GeoDataFrame, max_cellsize: int = 10, max_complexity: int = 10000\n",
    ") -> gpd.GeoDataFrame:\n",
    "    # Get the bounds of the GeoDataFrame\n",
    "    minx, miny, maxx, maxy = gdf.total_bounds\n",
    "\n",
    "    # Create an initial grid\n",
    "    grid_gdf = create_grid((minx, miny, maxx, maxy), max_cellsize)\n",
    "\n",
    "    # Subdivide grid elements based on density and complexity\n",
    "    subdivided_elements = subdivide_grid(grid_gdf, gdf, max_cellsize, max_complexity)\n",
    "\n",
    "    return gpd.GeoDataFrame(geometry=subdivided_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TODO: refactor this so old function mantains functionality for marine areas\n",
    "\n",
    "def split_gdf_by_grid(gdf: gpd.GeoDataFrame, grid_gdf: gpd.GeoDataFrame):\n",
    "    result = []\n",
    "    gdf[\"already_processed\"] = False\n",
    "    for geometry in grid_gdf.geometry:\n",
    "        candidates = get_matches(geometry, gdf)\n",
    "        subset = gdf.loc[candidates.index][~gdf[\"already_processed\"]]\n",
    "        gdf.loc[subset.index, \"already_processed\"] = True\n",
    "        if not subset.empty:\n",
    "            result.append(subset.drop(columns=[\"already_processed\"]).reset_index(drop=True).copy())\n",
    "    return result\n",
    "\n",
    "\n",
    "@background\n",
    "def spatial_join_chunk(df_large_chunk, df_small, pbar):\n",
    "    try:\n",
    "        bbox = df_large_chunk.total_bounds\n",
    "\n",
    "        candidates = get_matches(box(*bbox), df_small.geometry)\n",
    "        if len(candidates) > 0:\n",
    "            subset = df_small.loc[candidates.index].clip(box(*bbox))\n",
    "\n",
    "            result = (\n",
    "                df_large_chunk.sjoin(subset, how=\"inner\")\n",
    "                .clip(subset.geometry)\n",
    "                .reset_index(drop=True)\n",
    "            )\n",
    "            result.geometry = result.geometry.apply(repair_geometry)\n",
    "        else:\n",
    "            result = gpd.GeoDataFrame(columns=df_large_chunk.columns)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logging.error(e)\n",
    "        return gpd.GeoDataFrame()\n",
    "    finally:\n",
    "        pbar.update(1)\n",
    "\n",
    "\n",
    "async def spatial_join(\n",
    "    geodataframe_a: gpd.GeoDataFrame, geodataframe_b: gpd.GeoDataFrame\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"Create spatial join between two GeoDataFrames.\"\"\"\n",
    "    # we build the spatial index for the larger GeoDataFrame\n",
    "    smaller_dim, larger_dim = arrange_dimensions(geodataframe_a, geodataframe_b)\n",
    "\n",
    "    logger.info(f\"Processing {len(larger_dim)} elements\")\n",
    "\n",
    "    grid = create_density_based_grid(larger_dim, max_cellsize=10, max_complexity=5000)\n",
    "\n",
    "    logger.info(f\"grid created with {len(grid)} cells\")\n",
    "\n",
    "    list_of_chunks = split_gdf_by_grid(larger_dim, grid)\n",
    "\n",
    "    logger.info(f\"grid split into {len(list_of_chunks)} chunks\")\n",
    "\n",
    "    with tqdm(total=len(list_of_chunks)) as pbar:  # we create a progress bar\n",
    "        new_df = await asyncio.gather(\n",
    "            *(spatial_join_chunk(chunk, smaller_dim, pbar) for chunk in list_of_chunks)\n",
    "        )\n",
    "\n",
    "    return gpd.GeoDataFrame(pd.concat(new_df, ignore_index=True), crs=smaller_dim.crs)\n",
    "\n",
    "\n",
    "@background\n",
    "def spatial_dissolve_chunk(geometry, gdf, pbar):\n",
    "\n",
    "    try:\n",
    "        candidates = get_matches(\n",
    "            geometry,\n",
    "            gdf.geometry,\n",
    "        )\n",
    "        subset = gdf.loc[candidates.index]\n",
    "\n",
    "        result = pd.concat(\n",
    "            subset.clip(geometry).pipe(split_by_year, year_col=\"STATUS_YR\"), ignore_index=True\n",
    "        ).copy()\n",
    "\n",
    "        data_chunk = [\n",
    "            (\n",
    "                result[result[\"year\"] <= 2010]\n",
    "                .reset_index()\n",
    "                .pipe(calculate_area, \"area\", None)\n",
    "                .drop(columns=[\"geometry\"])\n",
    "            )\n",
    "        ]\n",
    "        for year in range(2011, 2025):\n",
    "            data_chunk.append(\n",
    "                result[result[\"year\"] <= year]\n",
    "                .dissolve(\n",
    "                    by=[\"iso_3\"],\n",
    "                )\n",
    "                .assign(year=year)\n",
    "                .reset_index()\n",
    "                .pipe(calculate_area, \"area\", None)\n",
    "                .drop(columns=[\"geometry\"])\n",
    "            )\n",
    "\n",
    "        return pd.concat(data_chunk, ignore_index=True)\n",
    "    except Exception as e:\n",
    "        logging.error(e)\n",
    "        return gpd.GeoDataFrame()\n",
    "    finally:\n",
    "        pbar.update(1)\n",
    "\n",
    "async def process_grid(gdf):\n",
    "    grid_gdf = create_density_based_grid(gdf, max_cellsize=10, max_complexity=5000)\n",
    "    with tqdm(total=grid_gdf.shape[0]) as pbar:\n",
    "        pbar = tqdm(total=len(grid_gdf), desc=\"Processing grid elements\")\n",
    "        result = await asyncio.gather(*[spatial_dissolve_chunk(geometry, gdf, pbar) for geometry in grid_gdf.geometry.values])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gadm = gpd.read_file(\"../data/gadm/processed/preprocess/gadm_preprocess.shp\").pipe(clean_geometries)\n",
    "wdpa = gpd.read_file(\n",
    "    \"../data/mpa-terrestrial/processed/preprocess/mpa-terrestrial_preprocess.shp\"\n",
    ").pipe(clean_geometries)\n",
    "gadm.sindex\n",
    "wdpa.sindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdpa_subset = wdpa[\n",
    "    ~(\n",
    "        (wdpa.bounds.minx < -181)\n",
    "        | (wdpa.bounds.miny < -91)\n",
    "        | (wdpa.bounds.maxx > 181)\n",
    "        | (wdpa.bounds.maxy > 91)\n",
    "    )\n",
    "].reset_index(drop=True)\n",
    "\n",
    "gadm_sync = await simplify_async(gadm)\n",
    "sjoin_gdf = await spatial_join(wdpa_subset, gadm_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that we have not produce duplicates\n",
    "sjoin_gdf.loc[sjoin_gdf.duplicated(subset=[\"WDPA_PID\", \"GID_0\"], keep=False)].sort_values(\n",
    "    \"WDPA_PID\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = await process_grid(sjoin_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_oecms = (\n",
    "    sjoin_gdf.groupby([\"iso_3\", \"PA_DEF\"])\n",
    "    .agg({\"PA_DEF\": \"count\"})\n",
    "    .rename(columns={\"PA_DEF\": \"count\"})\n",
    "    .reset_index()\n",
    "    .pivot(index=\"iso_3\", columns=\"PA_DEF\", values=\"count\")\n",
    "    .fillna(0)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"0\": \"oecm\", \"1\": \"pa\"})\n",
    ")\n",
    "# ).reset_index().pivot(index=\"iso_3\", columns=\"PA_DEF\", values=\"count\").reset_index(names=[\"PA_DEF\"], level=0, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_oecms[\"oecm_perc\"] = result_oecms[\"oecm\"] / (result_oecms[\"oecm\"] + result_oecms[\"pa\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>PA_DEF</th>\n",
       "      <th>iso_3</th>\n",
       "      <th>oecm</th>\n",
       "      <th>pa</th>\n",
       "      <th>oecm_perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>USA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50674.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>SWE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30813.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>DEU</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23703.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>EST</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20579.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>FIN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18427.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>CAN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12566.0</td>\n",
       "      <td>0.000159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>GBR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11712.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AUS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11154.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>CHE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10632.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>NZL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10205.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "PA_DEF iso_3  oecm       pa  oecm_perc\n",
       "180      USA   0.0  50674.0   0.000000\n",
       "161      SWE   0.0  30813.0   0.000000\n",
       "44       DEU   0.0  23703.0   0.000000\n",
       "55       EST   0.0  20579.0   0.000000\n",
       "57       FIN   0.0  18427.0   0.000000\n",
       "29       CAN   2.0  12566.0   0.000159\n",
       "61       GBR   0.0  11712.0   0.000000\n",
       "9        AUS   0.0  11154.0   0.000000\n",
       "30       CHE   0.0  10632.0   0.000000\n",
       "130      NZL   0.0  10205.0   0.000000"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_oecms.sort_values(\"pa\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_area = pd.concat(data)[['iso_3', 'year', 'area']].groupby(['iso_3', 'year']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result_area.merge(result_oecms, on=\"iso_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Todo: this needs to be merged with the marine data and validated with the pandera model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mresult\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "# Todo: this needs to be merged with the marine data and validated with the pandera model\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
