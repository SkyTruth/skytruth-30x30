{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: should we save every output as a [geoparquet](https://geoparquet.org/) in the future to improve read performance (reduction 30% read time)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging import getLogger\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "scripts_dir = Path(\"../..\").joinpath(\"src\")\n",
    "import sys\n",
    "if scripts_dir not in sys.path:\n",
    "    sys.path.insert(0, scripts_dir.resolve().as_posix())\n",
    "\n",
    "from helpers.utils import downloadFile, rm_tree, make_archive, writeReadGCP\n",
    "from helpers.settings import get_settings\n",
    "from helpers.file_handler import FileConventionHandler\n",
    "from pipelines.utils import watch\n",
    "from pipelines.processors import (\n",
    "    set_wdpa_id,\n",
    "    protection_level,\n",
    "    status,\n",
    "    create_year,\n",
    "    get_mpas,\n",
    "    set_location_iso,\n",
    "    set_fps_classes,\n",
    "    filter_by_methodology,\n",
    "    filter_by_terrestrial,\n",
    "    transform_points,\n",
    "    clean_geometries,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysettings = get_settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eez_intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipe params\n",
    "force_clean = True\n",
    "step = \"preprocess\"\n",
    "pipe = \"eez\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data sources\n",
    "## EEZ\n",
    "EEZ_url = \"https://www.marineregions.org/download_file.php\"\n",
    "EEZ_file_name = \"eez_v11.shp\"\n",
    "EEZ_params = {\"name\": \"World_EEZ_v11_20191118.zip\"}\n",
    "EEZ_headers = {\n",
    "    \"content-type\": \"application/x-www-form-urlencoded\",\n",
    "    \"cookie\": \"PHPSESSID=29190501b4503e4b33725cd6bd01e2c6; vliz_webc=vliz_webc2; jwplayer.captionLabel=Off\",\n",
    "    \"dnt\": \"1\",\n",
    "    \"origin\": \"https://www.marineregions.org\",\n",
    "    \"sec-fetch-dest\": \"document\",\n",
    "    \"sec-fetch-mode\": \"navigate\",\n",
    "    \"sec-fetch-site\": \"same-origin\",\n",
    "    \"sec-fetch-user\": \"?1\",\n",
    "    \"upgrade-insecure-requests\": \"1\",\n",
    "}\n",
    "\n",
    "EEZ_body = {\n",
    "    \"name\": \"Jason\",\n",
    "    \"organisation\": \"skytruth\",\n",
    "    \"email\": \"hello@skytruth.com\",\n",
    "    \"country\": \"Spain\",\n",
    "    \"user_category\": \"academia\",\n",
    "    \"purpose_category\": \"Conservation\",\n",
    "    \"agree\": \"1\",\n",
    "}\n",
    "\n",
    "## High seas\n",
    "hs_url = \"https://www.marineregions.org/download_file.php\"\n",
    "hs_file_name = \"High_seas_v1.shp\"\n",
    "hs_params = {\"name\": \"World_High_Seas_v1_20200826.zip\"}\n",
    "hs_headers = {\n",
    "    \"content-type\": \"application/x-www-form-urlencoded\",\n",
    "    \"cookie\": \"PHPSESSID=29190501b4503e4b33725cd6bd01e2c6; vliz_webc=vliz_webc2; jwplayer.captionLabel=Off\",\n",
    "    \"dnt\": \"1\",\n",
    "    \"origin\": \"https://www.marineregions.org\",\n",
    "    \"sec-fetch-dest\": \"document\",\n",
    "    \"sec-fetch-mode\": \"navigate\",\n",
    "    \"sec-fetch-site\": \"same-origin\",\n",
    "    \"sec-fetch-user\": \"?1\",\n",
    "    \"upgrade-insecure-requests\": \"1\",\n",
    "}\n",
    "hs_body = {\n",
    "    \"name\": \"Jason\",\n",
    "    \"organisation\": \"skytruth\",\n",
    "    \"email\": \"hello@skytruth.com\",\n",
    "    \"country\": \"Spain\",\n",
    "    \"user_category\": \"academia\",\n",
    "    \"purpose_category\": \"Conservation\",\n",
    "    \"agree\": \"1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_folder = FileConventionHandler(pipe)\n",
    "input_path = working_folder.pipe_raw_path\n",
    "temp_working_path = working_folder.get_temp_file_path(step)\n",
    "\n",
    "output_path = working_folder.get_processed_step_path(step)\n",
    "output_file = working_folder.get_step_fmt_file_path(step, \"shp\")\n",
    "zipped_output_file = working_folder.get_step_fmt_file_path(step, \"zip\", True)\n",
    "remote_path = working_folder.get_remote_path(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/mambauser/data/eez/raw/World_High_Seas_v1_20200826.zip')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract data\n",
    "## download files EEZ & High seas\n",
    "downloadFile(\n",
    "    EEZ_url,\n",
    "    input_path,\n",
    "    EEZ_body,\n",
    "    EEZ_params,\n",
    "    EEZ_headers,\n",
    "    overwrite=force_clean,\n",
    ")\n",
    "downloadFile(hs_url, input_path, hs_body, hs_params, hs_headers, overwrite=force_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mambauser/data/eez/raw/temp_preprocess/World_EEZ_v11_20191118\n",
      "/home/mambauser/data/eez/raw/temp_preprocess/World_High_Seas_v1_20200826\n"
     ]
    }
   ],
   "source": [
    "## unzip file if needed & load data\n",
    "unziped_folders = []\n",
    "for idx, path in enumerate(input_path.glob(\"*.zip\")):\n",
    "    unziped_folder = temp_working_path.joinpath(path.stem)\n",
    "    print(unziped_folder)\n",
    "\n",
    "    if unziped_folder.exists() and force_clean:\n",
    "        rm_tree(unziped_folder)\n",
    "\n",
    "    shutil.unpack_archive(path, unziped_folder.parent if idx == 0 else unziped_folder)\n",
    "\n",
    "    files = [gpd.read_file(file) for file in unziped_folder.rglob(\"*.shp\") if \"boundaries\" not in file.stem]\n",
    "    unziped_folders.append(\n",
    "        pd.concat(files)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data\n",
    "## set the same structure for both datasets updating the high seas one\n",
    "unziped_folders[1] = (\n",
    "    unziped_folders[1]\n",
    "    .rename(\n",
    "        columns={\"name\": \"GEONAME\", \"area_km2\": \"AREA_KM2\", \"mrgid\": \"MRGID\"},\n",
    "    )\n",
    "    .assign(\n",
    "        POL_TYPE=\"High Seas\",\n",
    "        ISO_SOV1=\"ABNJ\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# merge datasets\n",
    "df = pd.concat(unziped_folders, ignore_index=True)\n",
    "\n",
    "df.drop(\n",
    "    columns=list(\n",
    "        set(df.columns)\n",
    "        - set(\n",
    "            [\n",
    "                \"MRGID\",\n",
    "                \"GEONAME\",\n",
    "                \"POL_TYPE\",\n",
    "                \"ISO_SOV1\",\n",
    "                \"ISO_SOV2\",\n",
    "                \"ISO_SOV3\",\n",
    "                \"AREA_KM2\",\n",
    "                \"geometry\",\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "gpd.GeoDataFrame(\n",
    "    df,\n",
    "    crs=unziped_folders[0].crs,\n",
    ").to_file(filename=output_file.as_posix(), driver=\"ESRI Shapefile\")\n",
    "\n",
    "# zip data\n",
    "make_archive(output_path, zipped_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean unzipped files\n",
    "rm_tree(temp_working_path) if temp_working_path.exists() else None\n",
    "rm_tree(output_path) if output_path.exists() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD\n",
    "## load zipped file to GCS\n",
    "writeReadGCP(\n",
    "    credentials=mysettings.GCS_KEYFILE_JSON,\n",
    "    bucket_name=mysettings.GCS_BUCKET,\n",
    "    blob_name=remote_path,\n",
    "    file=zipped_output_file,\n",
    "    operation=\"w\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mpa Atlas intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_clean = True\n",
    "step = \"preprocess\"\n",
    "pipe = \"mpaatlas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data source\n",
    "mpaatlas_url = \"https://guide.mpatlas.org/api/v1/zone/geojson\"\n",
    "mpaatlas_file_name = \"mpatlas_assess_zone.geojson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_folder = FileConventionHandler(pipe)\n",
    "input_path = working_folder.pipe_raw_path\n",
    "temp_working_path = working_folder.get_temp_file_path(step)\n",
    "\n",
    "output_path = working_folder.get_processed_step_path(step)\n",
    "output_file = working_folder.get_step_fmt_file_path(step, \"shp\")\n",
    "zipped_output_file = working_folder.get_step_fmt_file_path(step, \"zip\", True)\n",
    "remote_path = working_folder.get_remote_path(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "input_file = downloadFile(\n",
    "    mpaatlas_url,\n",
    "    input_path,\n",
    "    overwrite=force_clean,\n",
    "    file=mpaatlas_file_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not force_clean and zipped_output_file.exists():\n",
    "    print(f\"File {zipped_output_file} already exists\")\n",
    "\n",
    "# Transform data\n",
    "gdf = gpd.read_file(input_file)\n",
    "\n",
    "df = (gdf\n",
    "      .pipe(set_wdpa_id)\n",
    "      .pipe(protection_level)\n",
    "      .pipe(status)\n",
    "      .pipe(create_year))\n",
    "\n",
    "df.drop(\n",
    "    columns=list(\n",
    "        set(df.columns)\n",
    "        - set(\n",
    "            [\n",
    "                \"wdpa_id\",\n",
    "                \"mpa_zone_id\", \n",
    "                \"name\",\n",
    "                \"designation\",\n",
    "                \"sovereign\",\n",
    "                \"establishment_stage\",\n",
    "                \"protection_mpaguide_level\",\n",
    "                \"protection_level\",\n",
    "                \"year\",\n",
    "                \"geometry\",\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "    inplace=True,\n",
    ")\n",
    "df.rename(columns={\"sovereign\": \"location_id\", \"wdpa_pid\": \"wdpa_id\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_160/3601108936.py:5: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  ).to_file(filename=output_file.as_posix(), driver=\"ESRI Shapefile\", encoding=\"utf-8\")\n",
      "/opt/conda/lib/python3.12/site-packages/pyogrio/raw.py:709: RuntimeWarning: Normalized/laundered field name: 'mpa_zone_id' to 'mpa_zone_i'\n",
      "  ogr_write(\n",
      "/opt/conda/lib/python3.12/site-packages/pyogrio/raw.py:709: RuntimeWarning: Normalized/laundered field name: 'designation' to 'designatio'\n",
      "  ogr_write(\n",
      "/opt/conda/lib/python3.12/site-packages/pyogrio/raw.py:709: RuntimeWarning: Normalized/laundered field name: 'location_id' to 'location_i'\n",
      "  ogr_write(\n",
      "/opt/conda/lib/python3.12/site-packages/pyogrio/raw.py:709: RuntimeWarning: Normalized/laundered field name: 'establishment_stage' to 'establishm'\n",
      "  ogr_write(\n",
      "/opt/conda/lib/python3.12/site-packages/pyogrio/raw.py:709: RuntimeWarning: Normalized/laundered field name: 'protection_mpaguide_level' to 'protection'\n",
      "  ogr_write(\n",
      "/opt/conda/lib/python3.12/site-packages/pyogrio/raw.py:709: RuntimeWarning: Normalized/laundered field name: 'protection_level' to 'protecti_1'\n",
      "  ogr_write(\n"
     ]
    }
   ],
   "source": [
    "#save data\n",
    "gpd.GeoDataFrame(\n",
    "    df,\n",
    "    crs=gdf.crs,\n",
    ").to_file(filename=output_file.as_posix(), driver=\"ESRI Shapefile\", encoding=\"utf-8\")\n",
    "\n",
    "make_archive(output_path, zipped_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD\n",
    "## load zipped file to GCS\n",
    "writeReadGCP(\n",
    "    credentials=mysettings.GCS_KEYFILE_JSON,\n",
    "    bucket_name=mysettings.GCS_BUCKET,\n",
    "    blob_name=remote_path,\n",
    "    file=zipped_output_file,\n",
    "    operation=\"w\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean unzipped files\n",
    "rm_tree(temp_working_path) if temp_working_path.exists() else None\n",
    "rm_tree(output_path) if output_path.exists() else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Protected seas intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPRECATED\n",
    "force_clean = True\n",
    "step = \"preprocess\"\n",
    "pipe = \"protectedseas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_csv_url = \"ProtectedSeas/ProtectedSeas_06142023.csv\"\n",
    "ps_csv_output = input_path.joinpath(ps_csv_url.split(\"/\")[-1])\n",
    "\n",
    "ps_geometries_url = (\n",
    "    \"ProtectedSeas/ProtectedSeas_ProtectedSeas_06142023_shp_ProtectedSeas_06142023_shp.zip\"\n",
    ")\n",
    "ps_geometries_output = input_path.joinpath(ps_geometries_url.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_folder = FileConventionHandler(pipe)\n",
    "input_path = working_folder.pipe_raw_path\n",
    "temp_working_path = working_folder.get_temp_file_path(step)\n",
    "\n",
    "output_path = working_folder.get_processed_step_path(step)\n",
    "output_file = working_folder.get_step_fmt_file_path(step, \"shp\")\n",
    "zipped_output_file = working_folder.get_step_fmt_file_path(step, \"zip\", True)\n",
    "remote_path = working_folder.get_remote_path(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not force_clean and zipped_output_file.exists():\n",
    "    print(f\"File {zipped_output_file} already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the data\n",
    "\n",
    "writeReadGCP(\n",
    "    credentials=mysettings.GCS_KEYFILE_JSON,\n",
    "    bucket_name=mysettings.GCS_BUCKET,\n",
    "    blob_name=ps_csv_url,\n",
    "    file=ps_csv_output,\n",
    "    operation=\"r\",\n",
    ")\n",
    "\n",
    "writeReadGCP(\n",
    "    credentials=mysettings.GCS_KEYFILE_JSON,\n",
    "    bucket_name=mysettings.GCS_BUCKET,\n",
    "    blob_name=ps_geometries_url,\n",
    "    file=ps_geometries_output,\n",
    "    operation=\"r\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip shapefile\n",
    "shutil.unpack_archive(ps_geometries_output, temp_working_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mambauser/src/pipelines/processors.py:77: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  return df[mask1][mask2].reset_index()\n"
     ]
    }
   ],
   "source": [
    "# transform data\n",
    "# TODO: Modify the preprocessing steps so we do not eliminate the geometries that does not intersect with MPAs - do to a change in the processing methodology\n",
    "data_table = pd.read_csv(ps_csv_output).pipe(get_mpas).pipe(set_location_iso).pipe(set_fps_classes)\n",
    "\n",
    "data_table.drop(\n",
    "    columns=data_table.columns.difference(\n",
    "        [\n",
    "            \"site_id\",\n",
    "            \"iso\",\n",
    "            \"FPS_cat\",\n",
    "            \"site_name\",\n",
    "            \"country\",\n",
    "            \"wdpa_id\",\n",
    "            \"removal_of_marine_life_is_prohibited\",\n",
    "            \"total_area\",\n",
    "        ]\n",
    "    ),\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "data_table.rename(columns={\"removal_of_marine_life_is_prohibited\": \"FPS\"}, inplace=True)\n",
    "\n",
    "# load geoemtries & merge\n",
    "\n",
    "gdf = gpd.read_file(ps_geometries_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "gdf.merge(data_table, how=\"inner\", left_on=\"SITE_ID\", right_on=\"site_id\").drop(\n",
    "    columns=[\"SITE_ID\", \"SITE_NAME\"]\n",
    ").to_file(filename=output_file.as_posix(), driver=\"ESRI Shapefile\", encoding=\"utf-8\")\n",
    "\n",
    "# zip data\n",
    "make_archive(output_path, zipped_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean unzipped files\n",
    "rm_tree(temp_working_path) if temp_working_path.exists() else None\n",
    "rm_tree(output_path) if output_path.exists() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD\n",
    "## load zipped file to GCS\n",
    "writeReadGCP(\n",
    "    credentials=mysettings.GCS_KEYFILE_JSON,\n",
    "    bucket_name=mysettings.GCS_BUCKET,\n",
    "    blob_name=remote_path,\n",
    "    file=zipped_output_file,\n",
    "    operation=\"w\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mpas protected planet intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_clean = True\n",
    "step = \"preprocess\"\n",
    "pipe = \"mpa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpa_url = \"https://www.protectedplanet.net/downloads\"\n",
    "mpa_body = {\n",
    "    \"domain\": \"general\",\n",
    "    \"format\": \"shp\",\n",
    "    \"token\": \"marine\",\n",
    "    \"id\": 21961,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_folder = FileConventionHandler(pipe)\n",
    "input_path = working_folder.pipe_raw_path\n",
    "temp_working_path = working_folder.get_temp_file_path(step)\n",
    "\n",
    "output_path = working_folder.get_processed_step_path(step)\n",
    "output_file = working_folder.get_step_fmt_file_path(step, \"shp\")\n",
    "zipped_output_file = working_folder.get_step_fmt_file_path(step, \"zip\", True)\n",
    "remote_path = working_folder.get_remote_path(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'marine-shp', 'title': 'WDPA_WDOECM_Aug2024_Public_marine_shp', 'url': 'https://d1gam3xoknrgr2.cloudfront.net/current/WDPA_WDOECM_Aug2024_Public_marine_shp.zip', 'hasFailed': False, 'token': 'marine'}\n"
     ]
    }
   ],
   "source": [
    "# download data\n",
    "r = requests.post(url=mpa_url, data=mpa_body)\n",
    "r.raise_for_status()\n",
    "\n",
    "download_url = r.json().get(\"url\")\n",
    "input_file_name = f'{r.json().get(\"title\")}.zip'\n",
    "print(r.json())\n",
    "\n",
    "input_file =  downloadFile(\n",
    "    url=download_url,\n",
    "    output_path=input_path,\n",
    "    overwrite=force_clean,\n",
    "    file=input_file_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip file twice due how data is provisioned by protected planet\n",
    "shutil.unpack_archive(\n",
    "    input_file,\n",
    "    temp_working_path,\n",
    "    \"zip\",\n",
    ")\n",
    "\n",
    "for file in temp_working_path.glob(\"*.zip\"):\n",
    "    shutil.unpack_archive(file, temp_working_path.joinpath(file.stem), \"zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data & Transform it\n",
    "unziped_folders = []\n",
    "for file in temp_working_path.glob(\"*/*.shp\"):\n",
    "    df = (\n",
    "        gpd.read_file(file)\n",
    "        .pipe(filter_by_methodology)\n",
    "        .pipe(transform_points)\n",
    "        .pipe(clean_geometries)\n",
    "    )\n",
    "    unziped_folders.append(df)\n",
    "\n",
    "# merge datasets\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    pd.concat(unziped_folders, ignore_index=True),\n",
    "    crs=unziped_folders[0].crs,\n",
    ")\n",
    "\n",
    "gdf.drop(\n",
    "    columns=list(\n",
    "        set(gdf.columns)\n",
    "        - set(\n",
    "            [\n",
    "                \"geometry\",\n",
    "                \"WDPAID\",\n",
    "                \"WDPA_PID\",\n",
    "                \"PA_DEF\",\n",
    "                \"NAME\",\n",
    "                \"PARENT_ISO\",\n",
    "                \"DESIG_ENG\",\n",
    "                \"IUCN_CAT\",\n",
    "                \"STATUS\",\n",
    "                \"STATUS_YR\",\n",
    "                \"GIS_M_AREA\",\n",
    "                \"AREA_KM2\",\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "    inplace=True,\n",
    ")\n",
    "gdf[\"WDPAID\"] = pd.to_numeric(gdf[\"WDPAID\"], downcast=\"integer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data & zip it\n",
    "gdf.to_file(filename=output_file, driver=\"ESRI Shapefile\", encoding=\"utf-8\")\n",
    "\n",
    "make_archive(output_path, zipped_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD\n",
    "## load zipped file to GCS\n",
    "writeReadGCP(\n",
    "    credentials=mysettings.GCS_KEYFILE_JSON,\n",
    "    bucket_name=mysettings.GCS_BUCKET,\n",
    "    blob_name=remote_path,\n",
    "    file=zipped_output_file,\n",
    "    operation=\"w\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean unzipped files\n",
    "rm_tree(temp_working_path) if temp_working_path.exists() else None\n",
    "rm_tree(output_path) if output_path.exists() else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mpas protected planet intermediate terrestrial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_clean = True\n",
    "step = \"preprocess\"\n",
    "pipe = \"mpa-terrestrial\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpa_url = \"https://www.protectedplanet.net/downloads\"\n",
    "mpa_body = {\n",
    "    \"domain\": \"general\",\n",
    "    \"format\": \"shp\",\n",
    "    \"token\": \"wdpa\",\n",
    "    \"id\": 76011,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_folder = FileConventionHandler(pipe)\n",
    "input_path = working_folder.pipe_raw_path\n",
    "temp_working_path = working_folder.get_temp_file_path(step)\n",
    "\n",
    "output_path = working_folder.get_processed_step_path(step)\n",
    "output_file = working_folder.get_step_fmt_file_path(step, \"gpkg\")\n",
    "zipped_output_file = working_folder.get_step_fmt_file_path(step, \"zip\", True)\n",
    "remote_path = working_folder.get_remote_path(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'wdpa-shp', 'title': 'WDPA_Sep2024_Public_shp', 'url': 'https://d1gam3xoknrgr2.cloudfront.net/current/WDPA_Sep2024_Public_shp.zip', 'hasFailed': False, 'token': 'wdpa'}\n"
     ]
    }
   ],
   "source": [
    "# download data\n",
    "r = requests.post(url=mpa_url, data=mpa_body)\n",
    "r.raise_for_status()\n",
    "\n",
    "download_url = r.json().get(\"url\")\n",
    "input_file_name = f'{r.json().get(\"title\")}.zip'\n",
    "print(r.json())\n",
    "\n",
    "input_file = downloadFile(\n",
    "    url=download_url,\n",
    "    output_path=input_path,\n",
    "    overwrite=force_clean,\n",
    "    file=input_file_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip file twice due how data is provisioned by protected planet\n",
    "shutil.unpack_archive(\n",
    "    input_file,\n",
    "    temp_working_path,\n",
    "    \"zip\",\n",
    ")\n",
    "\n",
    "for file in temp_working_path.glob(\"*.zip\"):\n",
    "    shutil.unpack_archive(file, temp_working_path.joinpath(file.stem), \"zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data & Transform it\n",
    "unziped_folders = []\n",
    "for file in temp_working_path.glob(\"*/*.shp\"):\n",
    "    df = (\n",
    "        gpd.read_file(file)\n",
    "        .pipe(filter_by_methodology)\n",
    "        .pipe(filter_by_terrestrial)\n",
    "        .pipe(transform_points)\n",
    "        .pipe(clean_geometries)\n",
    "    )\n",
    "    unziped_folders.append(df)\n",
    "\n",
    "# merge datasets\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    pd.concat(unziped_folders, ignore_index=True),\n",
    "    crs=unziped_folders[0].crs,\n",
    ")\n",
    "\n",
    "gdf.drop(\n",
    "    columns=list(\n",
    "        set(gdf.columns)\n",
    "        - set(\n",
    "            [\n",
    "                \"geometry\",\n",
    "                \"WDPAID\",\n",
    "                \"WDPA_PID\",\n",
    "                \"PA_DEF\",\n",
    "                \"NAME\",\n",
    "                \"PARENT_ISO\",\n",
    "                \"DESIG_ENG\",\n",
    "                \"IUCN_CAT\",\n",
    "                \"STATUS\",\n",
    "                \"STATUS_YR\",\n",
    "                \"GIS_AREA\",\n",
    "                \"MARINE\",\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "    inplace=True,\n",
    ")\n",
    "gdf[\"WDPAID\"] = pd.to_numeric(gdf[\"WDPAID\"], downcast=\"integer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data & zip it\n",
    "gdf.to_file(\n",
    "    filename=output_file,\n",
    "    driver=\"GPKG\",\n",
    "    layer=\"name\",\n",
    "    encoding=\"utf-8\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# LOAD\n",
    "## load zipped file to GCS\n",
    "writeReadGCP(\n",
    "    credentials=mysettings.GCS_KEYFILE_JSON,\n",
    "    bucket_name=mysettings.GCS_BUCKET,\n",
    "    blob_name=remote_path,\n",
    "    file=output_file,\n",
    "    operation=\"w\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean unzipped files\n",
    "rm_tree(temp_working_path) if temp_working_path.exists() else None\n",
    "rm_tree(output_path) if output_path.exists() else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Habitats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_clean = True\n",
    "step = \"preprocess\"\n",
    "pipe = \"habitats\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "habitats_download_url = \"https://habitats.oceanplus.org/downloads/global_statistics.zip\"\n",
    "Mangroves_download_url = \"https://mangrove-atlas-api.herokuapp.com/admin/widget_protected_areas.csv\"\n",
    "mangroves_request_headers = {\n",
    "    \"Cookie\": \"_mangrove_atlas_api_session=fJuobvI2fH42WfGfMtRTp%2BksIDdPEpY6DG8uCuITsENtrRGG4AA3nYEeAI7dytzpK%2F0dGIHq84O54MRr6eiPgiwCYXp2XP4IzXM40dFt%2FI6hoB0WXC%2Fwrd81XreNnMZiSEE6IVT5R0fqMcmsZdPn53u0A1d4CGU3FfliOZuWkckBuA%2F7C4upBGuSS8817LqOh1slG%2BsEOGp3nk7WX4fMoPbsHWtARfFwdfoAHz448LO7uWuZdyiu7YOrS0ZxOZEb9JZ8hcUJph4pBFofZLpOvtQQutgZY21T5bhQ7Kwfl56e6Qr0SZ%2B8sIzMfky3h%2FjOA6DNTLoy%2BZLiZBAgFHlTYm2JwlwqWgAZU8D7cE7Zn%2Fxgf3LFF9pZ9Fe3QG4c8LIwH%2FxqjEd8GsZAhBMgBWbxubigQ9gZssZt6CIO--7qiVsTAT8JAKj1jU--U7TI%2Fz9c151bfD8iZdkBDw%3D%3D\"\n",
    "}\n",
    "seamounts_download_url = \"https://datadownload-production.s3.amazonaws.com/ZSL002_ModelledSeamounts2011_v1.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_folder = FileConventionHandler(pipe)\n",
    "input_path = working_folder.pipe_raw_path\n",
    "temp_working_path = working_folder.get_temp_file_path(step)\n",
    "\n",
    "output_path = working_folder.get_processed_step_path(step)\n",
    "output_file = working_folder.get_step_fmt_file_path(step, \"shp\")\n",
    "zipped_output_file = working_folder.get_step_fmt_file_path(step, \"zip\", True)\n",
    "remote_path = working_folder.get_remote_path(step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seamounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seamounts_path = input_path.joinpath(\"seamounts\")\n",
    "input_seamounts_path.mkdir(parents=True, exist_ok=True)\n",
    "# download data\n",
    "input_file_name = \"seamounts.zip\"\n",
    "input_file = downloadFile(\n",
    "    url=seamounts_download_url,\n",
    "    output_path=input_seamounts_path,\n",
    "    overwrite=force_clean,\n",
    "    file=input_file_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip data\n",
    "shutil.unpack_archive(\n",
    "    input_file,\n",
    "    temp_working_path,\n",
    "    \"zip\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/mambauser/data/habitats/raw/temp_preprocess')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_working_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "first =gpd.read_file(next(temp_working_path.rglob(\"*SeamountsBaseArea.shp\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PEAKID</th>\n",
       "      <th>DEPTH</th>\n",
       "      <th>HEIGHT</th>\n",
       "      <th>LONG</th>\n",
       "      <th>LAT</th>\n",
       "      <th>AREA2D</th>\n",
       "      <th>FILTER</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26000</td>\n",
       "      <td>-2547</td>\n",
       "      <td>1148</td>\n",
       "      <td>2.762500</td>\n",
       "      <td>84.979736</td>\n",
       "      <td>982.028337</td>\n",
       "      <td>0</td>\n",
       "      <td>POLYGON ((2.91249 84.82976, 2.76249 84.79636, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26157</td>\n",
       "      <td>-3084</td>\n",
       "      <td>1296</td>\n",
       "      <td>9.143056</td>\n",
       "      <td>84.935292</td>\n",
       "      <td>348.473055</td>\n",
       "      <td>0</td>\n",
       "      <td>POLYGON ((9.99309 84.93526, 9.25139 84.82696, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26158</td>\n",
       "      <td>-3043</td>\n",
       "      <td>1342</td>\n",
       "      <td>9.183333</td>\n",
       "      <td>84.938070</td>\n",
       "      <td>367.540380</td>\n",
       "      <td>0</td>\n",
       "      <td>POLYGON ((9.07499 85.04636, 9.18329 85.03806, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26228</td>\n",
       "      <td>-3142</td>\n",
       "      <td>1379</td>\n",
       "      <td>8.748611</td>\n",
       "      <td>84.907514</td>\n",
       "      <td>299.443636</td>\n",
       "      <td>0</td>\n",
       "      <td>POLYGON ((9.79859 84.90756, 8.83199 84.82416, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26229</td>\n",
       "      <td>-3146</td>\n",
       "      <td>1383</td>\n",
       "      <td>8.887500</td>\n",
       "      <td>84.913070</td>\n",
       "      <td>309.588492</td>\n",
       "      <td>0</td>\n",
       "      <td>POLYGON ((8.88749 84.83806, 8.81249 84.83806, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33447</th>\n",
       "      <td>4999430</td>\n",
       "      <td>-298</td>\n",
       "      <td>1376</td>\n",
       "      <td>-142.295833</td>\n",
       "      <td>-74.566097</td>\n",
       "      <td>819.608801</td>\n",
       "      <td>0</td>\n",
       "      <td>POLYGON ((-142.29582 -74.72444, -142.46251 -74...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33448</th>\n",
       "      <td>4999462</td>\n",
       "      <td>-295</td>\n",
       "      <td>1274</td>\n",
       "      <td>-142.250000</td>\n",
       "      <td>-74.570264</td>\n",
       "      <td>777.598079</td>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((-142.25001 -74.72864, -142.41671 -74...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33449</th>\n",
       "      <td>4999913</td>\n",
       "      <td>-348</td>\n",
       "      <td>3288</td>\n",
       "      <td>-164.179167</td>\n",
       "      <td>-74.766097</td>\n",
       "      <td>1000.023088</td>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((-164.01251 -74.93274, -164.17921 -74...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33450</th>\n",
       "      <td>5000862</td>\n",
       "      <td>-2739</td>\n",
       "      <td>1060</td>\n",
       "      <td>-158.162500</td>\n",
       "      <td>-75.141097</td>\n",
       "      <td>814.426234</td>\n",
       "      <td>0</td>\n",
       "      <td>POLYGON ((-158.16251 -75.28274, -158.30421 -75...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33451</th>\n",
       "      <td>5000879</td>\n",
       "      <td>-2710</td>\n",
       "      <td>1104</td>\n",
       "      <td>-158.116667</td>\n",
       "      <td>-75.145264</td>\n",
       "      <td>762.518328</td>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((-158.11671 -75.27864, -158.25001 -75...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33452 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        PEAKID  DEPTH  HEIGHT        LONG        LAT       AREA2D  FILTER  \\\n",
       "0        26000  -2547    1148    2.762500  84.979736   982.028337       0   \n",
       "1        26157  -3084    1296    9.143056  84.935292   348.473055       0   \n",
       "2        26158  -3043    1342    9.183333  84.938070   367.540380       0   \n",
       "3        26228  -3142    1379    8.748611  84.907514   299.443636       0   \n",
       "4        26229  -3146    1383    8.887500  84.913070   309.588492       0   \n",
       "...        ...    ...     ...         ...        ...          ...     ...   \n",
       "33447  4999430   -298    1376 -142.295833 -74.566097   819.608801       0   \n",
       "33448  4999462   -295    1274 -142.250000 -74.570264   777.598079       1   \n",
       "33449  4999913   -348    3288 -164.179167 -74.766097  1000.023088       1   \n",
       "33450  5000862  -2739    1060 -158.162500 -75.141097   814.426234       0   \n",
       "33451  5000879  -2710    1104 -158.116667 -75.145264   762.518328       1   \n",
       "\n",
       "                                                geometry  \n",
       "0      POLYGON ((2.91249 84.82976, 2.76249 84.79636, ...  \n",
       "1      POLYGON ((9.99309 84.93526, 9.25139 84.82696, ...  \n",
       "2      POLYGON ((9.07499 85.04636, 9.18329 85.03806, ...  \n",
       "3      POLYGON ((9.79859 84.90756, 8.83199 84.82416, ...  \n",
       "4      POLYGON ((8.88749 84.83806, 8.81249 84.83806, ...  \n",
       "...                                                  ...  \n",
       "33447  POLYGON ((-142.29582 -74.72444, -142.46251 -74...  \n",
       "33448  POLYGON ((-142.25001 -74.72864, -142.41671 -74...  \n",
       "33449  POLYGON ((-164.01251 -74.93274, -164.17921 -74...  \n",
       "33450  POLYGON ((-158.16251 -75.28274, -158.30421 -75...  \n",
       "33451  POLYGON ((-158.11671 -75.27864, -158.25001 -75...  \n",
       "\n",
       "[33452 rows x 8 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not force_clean and zipped_output_file.exists():\n",
    "    print(f\"File {zipped_output_file} already exists\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
