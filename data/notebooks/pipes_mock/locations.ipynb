{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import json\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "scripts_dir = Path(\".\").joinpath(\"src\")\n",
    "if scripts_dir not in sys.path:\n",
    "    sys.path.insert(0, scripts_dir.resolve().as_posix())\n",
    "\n",
    "from helpers.strapi import Strapi\n",
    "from helpers.settings import get_settings, Settings\n",
    "from helpers.file_handler import FileConventionHandler\n",
    "from helpers.utils import download_and_unzip_if_needed, writeReadGCP\n",
    "\n",
    "from pipelines.output_schemas import (\n",
    "    FPLSchema,\n",
    "    ProtectionLevelSchema,\n",
    "    MPAsSchema,\n",
    "    HabitatsSchema,\n",
    "    LocationSchema,\n",
    "    ProtectedAreaExtentSchema,\n",
    ")\n",
    "from pipelines.processors import (\n",
    "    add_envelope,\n",
    "    add_location_iso,\n",
    "    expand_multiple_locations,\n",
    "    add_region_iso,\n",
    "    calculate_eez_area,\n",
    "    add_bbox,\n",
    "    add_groups_and_members,\n",
    "    add_location_name,\n",
    "    output,\n",
    "    clean_geometries,\n",
    "    filter_by_exluding_propossed_mpas,\n",
    "    spatial_join,\n",
    "    process_mpa_data,\n",
    "    assign_iso3,\n",
    "    calculate_global_area,\n",
    "    separate_parent_iso,\n",
    "    calculate_stats_cov,\n",
    "    coverage_stats,\n",
    "    mpaatlas_filter_stablishment,\n",
    "    process_mpaatlas_data,\n",
    "    calculate_stats,\n",
    "    fix_monaco,\n",
    "    batch_export,\n",
    "    calculate_area,\n",
    "    define_is_child,\n",
    "    set_child_id,\n",
    "    add_child_parent_relationship,\n",
    "    columns_to_lower,\n",
    "    extract_wdpaid_mpaatlas,\n",
    ")\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logging.getLogger(\"requests\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"fiona\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysettings = get_settings()\n",
    "prev_step = \"preprocess\"\n",
    "current_step = \"stats\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sofia/dev/skytruth-30x30/data/data/eez/processed/eez_preprocess.zip\n",
      "/home/sofia/dev/skytruth-30x30/data/data/eez/processed/preprocess\n",
      "/home/sofia/dev/skytruth-30x30/data/data/gadm/processed/gadm_preprocess.zip\n",
      "/home/sofia/dev/skytruth-30x30/data/data/gadm/processed/preprocess\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/sofia/dev/skytruth-30x30/data/data/gadm/processed/preprocess')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_eez = \"eez\"\n",
    "pipe_eez_dir = FileConventionHandler(pipe_eez)\n",
    "pipe_gadm = \"gadm\"\n",
    "pipe_gadm_dir = FileConventionHandler(pipe_gadm)\n",
    "\n",
    "output_file = pipe_gadm_dir.get_processed_step_path(current_step).joinpath(\"locations_all.json\")\n",
    "\n",
    "# Download the EEZ file && unzip it\n",
    "download_and_unzip_if_needed(pipe_eez_dir, prev_step, mysettings)\n",
    "\n",
    "# Download the gadm file && unzip it\n",
    "download_and_unzip_if_needed(pipe_gadm_dir, prev_step, mysettings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download country translations\n",
    "working_folder = FileConventionHandler(pipe_gadm)\n",
    "input_path = working_folder.pipe_raw_path\n",
    "input_path\n",
    "\n",
    "translations_csv_url = \"vizzuality_processed_data/gadm/preprocess/locations_translated.csv\"\n",
    "translations_csv_output = input_path.joinpath(translations_csv_url.split(\"/\")[-1])\n",
    "\n",
    "# writeReadGCP(\n",
    "#     credentials=mysettings.GCS_KEYFILE_JSON,\n",
    "#     bucket_name=mysettings.GCS_BUCKET,\n",
    "#     blob_name=translations_csv_url,\n",
    "#     file=translations_csv_output,\n",
    "#     operation=\"r\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "import pandera as pa\n",
    "from pandera.typing import Index, Series\n",
    "import pandas as pd\n",
    "\n",
    "class LocationSchemaAll(pa.DataFrameModel):\n",
    "    id: Index[int] = pa.Field(gt=0, coerce=True)\n",
    "    code: Series[str] = pa.Field(coerce=True)\n",
    "    name: Series[str] = pa.Field(coerce=True)\n",
    "    name_es: Series[str] = pa.Field(coerce=True)\n",
    "    name_fr: Series[str] = pa.Field(coerce=True)\n",
    "    total_marine_area: Series[float] = pa.Field(ge=0, coerce=True)  # noqa: N815\n",
    "    total_terrestrial_area: Series[float] = pa.Field(ge=0, coerce=True)  # noqa: N815\n",
    "    type: Series[str] = pa.Field(\n",
    "        unique_values_eq=[\"country\", \"worldwide\", \"region\", \"highseas\"], coerce=True\n",
    "    )\n",
    "    groups: Series[List[int]] = pa.Field(coerce=True)\n",
    "    marine_bounds: Series[List[float]] = pa.Field(coerce=True, nullable=True)\n",
    "    terrestrial_bounds: Series[List[float]] = pa.Field(coerce=True, nullable=True)\n",
    "\n",
    "def add_translations(df, translations_csv_path):\n",
    "    translations_df = pd.read_csv(translations_csv_path, keep_default_na=False, na_values=[])\n",
    "    \n",
    "    df = df.merge(translations_df[['code', 'name_es', 'name_fr']], left_on='iso', right_on='code', how='left')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_gadm_area(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    glob = gpd.GeoDataFrame(\n",
    "        {\n",
    "            \"iso\": \"GLOB\",\n",
    "            \"AREA_KM2\": 134954835,\n",
    "            \"location_type\": \"worldwide\",\n",
    "            \"region\": np.nan,\n",
    "            \"geometry\": gpd.GeoSeries([gpd.GeoSeries(df[\"geometry\"]).unary_union]),\n",
    "        },\n",
    "        crs=\"EPSG:4326\",\n",
    "    )\n",
    "\n",
    "    terrestrial_areas = (\n",
    "        df\n",
    "        .dissolve(by=[\"iso\", \"region\"], aggfunc={\"AREA_KM2\": \"sum\"})\n",
    "        .reset_index()\n",
    "        .assign(location_type=\"country\")\n",
    "    )\n",
    "    regions_areas = (\n",
    "        df\n",
    "        .dissolve(by=[\"region\"], aggfunc={\"AREA_KM2\": \"sum\"})\n",
    "        .reset_index()\n",
    "        .rename(columns={\"region\": \"iso\"})\n",
    "        .assign(location_type=\"region\")\n",
    "    )\n",
    "    result = (\n",
    "        pd.concat(\n",
    "            [\n",
    "                glob,\n",
    "                regions_areas,\n",
    "                terrestrial_areas,\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "        .dropna(subset=[\"iso\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    result.index = result.index + 1\n",
    "    result.index.name = \"id\"\n",
    "\n",
    "    return result.assign(id=result.index)\n",
    "\n",
    "def add_groups_and_members_land(df: pd.DataFrame | gpd.GeoDataFrame) -> pd.DataFrame | gpd.GeoDataFrame:\n",
    "    return df.assign(\n",
    "        groups=lambda row: row[[\"region\", \"location_type\"]].apply(\n",
    "            lambda x: (np.where(df.iso == x[\"region\"])[0] + 2).tolist()\n",
    "            if x[\"location_type\"] == \"country\"\n",
    "            else [],\n",
    "            axis=1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# def combine_bounds(marine_bounds, land_bounds):\n",
    "#     # Check if marine bounds are valid\n",
    "#     if isinstance(marine_bounds, list) and len(marine_bounds) == 4:\n",
    "#         return marine_bounds\n",
    "#     # If marine bounds are not valid, check land bounds\n",
    "#     elif isinstance(land_bounds, list) and len(land_bounds) == 4:\n",
    "#         return land_bounds\n",
    "#     # If neither bounds are valid, return an empty list\n",
    "#     else:\n",
    "#         return []\n",
    "\n",
    "def combine_columns(df, col1, col2, new_col):\n",
    "    \"\"\"\n",
    "    Combine two columns in a DataFrame using combine_first and assign to a new column.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the columns to combine.\n",
    "    col1 (str): The name of the first column.\n",
    "    col2 (str): The name of the second column.\n",
    "    new_col (str): The name of the new column to assign the combined result.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with the new combined column.\n",
    "    \"\"\"\n",
    "    df[new_col] = df[col1].combine_first(df[col2])\n",
    "    return df\n",
    "\n",
    "def add_region_iso_2(\n",
    "    df: pd.DataFrame | gpd.GeoDataFrame, iso_column\n",
    ") -> pd.DataFrame | gpd.GeoDataFrame:\n",
    "    regions = pd.read_json(scripts_dir.joinpath(\"data_commons/data/regions_data2.json\"))\n",
    "\n",
    "    def find_region_iso(iso: str) -> Union[str, None]:\n",
    "        filtered_regions = list(filter(lambda x: iso in x[\"country_iso_3s\"], regions.get(\"data\")))\n",
    "        return filtered_regions[0][\"region_iso\"] if len(filtered_regions) > 0 else None\n",
    "\n",
    "    return df.assign(region=lambda row: row[iso_column].apply(find_region_iso))\n",
    "\n",
    "def add_location_name_2(df: pd.DataFrame | gpd.GeoDataFrame) -> pd.DataFrame | gpd.GeoDataFrame:\n",
    "    with open(scripts_dir.joinpath('data_commons/data/iso_map2.json'), 'r') as f:\n",
    "        iso_map = json.load(f)\n",
    "\n",
    "    def get_name(iso):\n",
    "        test = iso_map.get(iso, np.nan)\n",
    "        return test\n",
    "\n",
    "    return df.assign(name=df.iso.apply(get_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: gadm includes some extra iso codes that had to be included in the regions_data.json (provided by protected planet) to process the terrestrial stats:\n",
    "\n",
    "'XCA': Caspian Sea, included in Asia & Pacific region\n",
    "\n",
    "'XKO': Kosovo, included in Europe region\n",
    "\n",
    "'ZNC': Northern Cyprus, included in Europe region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3524857/673975959.py:35: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  \"geometry\": gpd.GeoSeries([gpd.GeoSeries(df[\"geometry\"]).unary_union]),\n"
     ]
    }
   ],
   "source": [
    "# Process EEZ data (marine data)\n",
    "locations = (\n",
    "    gpd.read_file(pipe_eez_dir.get_step_fmt_file_path(prev_step, \"shp\"))\n",
    "    .pipe(add_envelope)\n",
    "    .pipe(add_location_iso)\n",
    "    .pipe(expand_multiple_locations)\n",
    "    .pipe(add_region_iso, 'iso')\n",
    "    .pipe(calculate_eez_area)\n",
    "    .pipe(add_bbox)\n",
    "    .pipe(add_groups_and_members)\n",
    "    .pipe(add_location_name)\n",
    "    .pipe(add_translations, translations_csv_output)\n",
    "    .rename(\n",
    "        columns={\n",
    "           \n",
    "            \"AREA_KM2\": \"total_marine_area\",\n",
    "            \"location_type\": \"type\",\n",
    "            \"bounds\":'marine_bounds'\n",
    "        }\n",
    "    )\n",
    ").reset_index(drop=True)\n",
    "\n",
    "locations.drop(\n",
    "    columns=list(\n",
    "        set(locations.columns) -\n",
    "        set([\"code\", \"name\", \"name_es\", \"name_fr\", \"total_marine_area\", \"marine_bounds\", \"type\", \"groups\", \"id\"])\n",
    "    ),\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "\n",
    "id_lookup = locations.set_index('code')['id'].to_dict() # Create a lookup dictionary for IDs from EEZ data\n",
    "\n",
    "# Process GADM data \n",
    "locations_land = (\n",
    "    gpd.read_file(pipe_gadm_dir.get_step_fmt_file_path(prev_step, \"shp\"))\n",
    "    .rename(columns={\"GID_0\": \"iso\", 'area_km2': 'AREA_KM2'})\n",
    "    .pipe(add_envelope)\n",
    "    .pipe(add_region_iso_2, 'iso') # add_region_iso_2 is used instead of add_region_iso because gadm includes new iso codes\n",
    "    .pipe(calculate_gadm_area)\n",
    "    .pipe(add_bbox)\n",
    "    .pipe(add_groups_and_members_land)\n",
    "    .pipe(add_location_name_2)\n",
    "    .pipe(add_translations, translations_csv_output)\n",
    "    .rename(\n",
    "        columns={\n",
    "            \"AREA_KM2\": \"total_terrestrial_area\",\n",
    "            \"location_type\": \"type\",\n",
    "            \"bounds\": \"terrestrial_bounds\"\n",
    "        }\n",
    "    )\n",
    ").reset_index(drop=True)\n",
    "\n",
    "locations_land['id'] = locations_land['code'].map(id_lookup) # Apply the EEZ IDs to the GADM dataset\n",
    "\n",
    "nan_mask = locations_land['id'].isna() # Identify the NaN values in the id column\n",
    "\n",
    "new_ids = pd.Series(\n",
    "    range(max(id_lookup.values()) + 1, max(id_lookup.values()) + 1 + nan_mask.sum()),\n",
    "    index=locations_land[nan_mask].index\n",
    ") # Generate new IDs for any GADM rows without an EEZ match\n",
    "\n",
    "locations_land['id'] = locations_land['id'].fillna(new_ids).astype(int) # Assign the new IDs to the NaN values in the id column\n",
    "\n",
    "locations_land.drop(\n",
    "    columns=list(\n",
    "        set(locations_land.columns) -\n",
    "        set([\"code\", \"name\", \"name_es\", \"name_fr\", \"total_terrestrial_area\", \"type\", \"groups\", \"terrestrial_bounds\", \"id\"])\n",
    "    ),\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# Merge EEZ and GADM datasets\n",
    "combined_locations = pd.merge(\n",
    "    locations, locations_land,\n",
    "    on=['code', 'id'],\n",
    "    suffixes=('_marine', '_land'),\n",
    "    how='outer'  \n",
    ")\n",
    "\n",
    "# Combine data from land and marine for each base column\n",
    "base_columns = ['type', 'groups', 'name', 'name_es', 'name_fr']\n",
    "for base_col in base_columns:\n",
    "    marine_col = f\"{base_col}_marine\"\n",
    "    land_col = f\"{base_col}_land\"\n",
    "    combined_locations = combine_columns(combined_locations, marine_col, land_col, base_col)\n",
    "\n",
    "# Fill NaN values with 0 for each column\n",
    "columns_to_fill = ['total_marine_area', 'total_terrestrial_area']\n",
    "for col in columns_to_fill:\n",
    "    combined_locations[col] = combined_locations[col].fillna(0)\n",
    "\n",
    "# Force the id column to be an integer\n",
    "combined_locations['id'] = combined_locations['id'].astype(int)\n",
    "\n",
    "\n",
    "# Drop unnecessary columns\n",
    "combined_locations.drop(\n",
    "    columns=[col for col in combined_locations.columns if col.endswith('_marine') or col.endswith('_land')],\n",
    "    inplace=True\n",
    ")\n",
    "combined_locations = combined_locations.reset_index(drop=True)\n",
    "\n",
    "# Force the index to have the values in id column (so they follow the same order in the previous table)\n",
    "combined_locations['index'] = combined_locations['id']\n",
    "combined_locations.set_index('index', inplace=True)\n",
    "combined_locations.sort_index(inplace=True)\n",
    "\n",
    "# Prepare final JSON output (stored in gadm folder)\n",
    "output_locations_combined = {\n",
    "    \"version\": 2,\n",
    "    \"data\": {\n",
    "        \"api::location.location\": LocationSchemaAll(pd.DataFrame(combined_locations)).to_dict(\n",
    "            orient=\"index\"\n",
    "        )\n",
    "    },\n",
    "}\n",
    "\n",
    "# Write the output to a JSON file (stored in gadm folder)\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(output_locations_combined, f)\n",
    "\n",
    "del output_locations_combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create locations_code (stored in gadm folder)\n",
    "(combined_locations[['id', 'code']].rename(columns={'id': 'location'})\n",
    " .to_csv(pipe_gadm_dir.get_processed_step_path(current_step)\n",
    "     .joinpath('locations_code_all.csv'), index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save locations_code in data_commons/data folder\n",
    "(combined_locations[['id', 'code']].rename(columns={'id': 'location'})\n",
    " .to_csv(scripts_dir.joinpath('data_commons/data/locations_code_all.csv'), index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:google.auth.transport.requests:Making request: POST https://oauth2.googleapis.com/token\n"
     ]
    }
   ],
   "source": [
    "# Upload csv to bucket\n",
    "remote_path = 'vizzuality_processed_data/strapi_tables/location_code.csv'\n",
    "\n",
    "writeReadGCP(\n",
    "    credentials=mysettings.GCS_KEYFILE_JSON,\n",
    "    bucket_name=mysettings.GCS_BUCKET,\n",
    "    blob_name=remote_path,\n",
    "    file=scripts_dir.joinpath('data_commons/data/locations_code_all.csv'),\n",
    "    operation=\"w\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
